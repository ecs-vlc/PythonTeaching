{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Advanced NumPy Modelling Examples\n",
    "\n",
    "This notebook will involve no direct teaching - instead we are going to attempt to solve a number of difficult problems. These problems will attempt to range over a number of inter-disciplinary fields. Don't worry if you are not able to complete them all within the time of the workshop - they are meant to stretch your abilities, gain some useful NumPy experience and grow some inter-disciplinary knowledge. For these tasks we require that you **only** use NumPy arrays as this is considerably faster and the only tractable method in later examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "## Monte Carlo Integration\n",
    "\n",
    "*Monte Carlo methods* rely primarily on repeated sampling to obtain numerical results, and are used extensively in any application that involves a probabilistic interpretation. They are particularly powerful in *high-dimensional* problems where deterministic methods for high dimensions become intractable. They are often coupled with other techniques such as **Markov Chains** once the probability distribution of a variable is parameterized.\n",
    "\n",
    "Let's say we have a mathematical function that is difficult to integrate:\n",
    "\n",
    "$$\n",
    "f(x)=\\sin^2 \\left(\\frac{1}{x(2-x)}\\right)\n",
    "$$\n",
    "\n",
    "to see this, do a quick plot.\n",
    "\n",
    "We see that the infinite oscillation as $f(x)$ draws towards 0 and 2 makes this function incredibly difficult to integrate using standard numerical methods (composite trapezoidal, simpsons).\n",
    "\n",
    "However we can make use of the fact that this function is *bounded* at [0, 2], and **scatter** a large uniform random distribution $\\cal N[0, 1]$ across this box. The fraction of them falling below the curve is approximately the integral we want to compute; hence:\n",
    "\n",
    "$$\n",
    "I=\\int_a^b f(x) \\ dx \\quad \\implies \\quad I \\simeq \\frac{k A}{N}\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of points considered, $k$ is the number falling below the curve, and $A$ is the area of the box. We know that $x \\in [0, 2]$, and we choose $y \\in [0, 1]$, giving $A=(y_1-y_0)(x_1-x_0)$.\n",
    "\n",
    "### Task 1.\n",
    "\n",
    "Write a function `monte_carlo_integrate()` which receives the function $f(x)$, the domain of $x$, the domain of $y$, and $N$. It should compute the integral of `f(x)` using the Monte Carlo method. Remember that the random numbers generated *must* be scaled into the domains of $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "\n",
    "Call `monte_carlo_integrate()` with the function described above, with $x \\in [0, 2]$, $y \\in [0, 1]$, and $N=10^5$. Is it reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3.\n",
    "\n",
    "The area of a circle of radius 2 is $4\\pi$. To help check the accuracy, let's calculate $\\pi$ by calculating the area of a *quarter-circle* in $x,y \\in [0, 2]$:\n",
    "\n",
    "$$\n",
    "\\pi = \\int_0^2 \\sqrt{4-x^2} \\ dx\n",
    "$$\n",
    "\n",
    "Repeat calls to `monte_carlo_integrate()` and generate integrals of Monte-Carlo integration with $N$. We recommend using $N=100 \\times 2^i$ for $i=0,\\dots,15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.\n",
    "\n",
    "Generate the loglog plot of $N$ against $E$, also known as the **convergence** of the algorithm, where:\n",
    "\n",
    "$$\n",
    "E= \\lvert \\pi-\\hat \\pi \\rvert\n",
    "$$\n",
    "\n",
    "where $\\hat \\pi = I$ integral from our Monte-Carlo method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.\n",
    "\n",
    "The coefficients for the slope (m) and intercept (b) can be calculated with `np.polyfit(N,E,1)`, remember to pass the $\\log N$ and $\\log E$! Plot the slope line through the points using loglog, using $y=mx+b$ for the $y$ variables. However given that the plot is in logspace, we calculate $y$ as:\n",
    "\n",
    "$$\n",
    "y=\\exp(b)N^m\n",
    "$$\n",
    "\n",
    "Calculate and plot the slope of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "For many real-world applications, given a set of data $X$ which we consider inputs or measurements observed, and $y$ which is considered an *continuous* output, or useful metric to inter, is there a set of latent coefficients/weights $w$ which when scaled to $X$ can infer $y$? This is the fundamental principle of **linear regression**. In this notation, capital letters $X$ describe *matrices*, and lower-case letters *w,y* describe vectors, with greek letters describing scalar coefficients. Linear models are henceforth:\n",
    "\n",
    "$$\n",
    "\\bf y=w^Tx+\\eta+\\epsilon\n",
    "$$\n",
    "\n",
    "where $w$ is our slope/gradient, $x$ is the input, $\\eta$ is the intercept and $\\epsilon$ is the error.\n",
    "\n",
    "We will however work in matrix notation, and to somplify the math we merge the intercept $\\eta$ into the weights $w$, and add a bias column to $X$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=\\left[\\begin{matrix}\n",
    "   x_{11} & x_{12} & \\dots & x_{1m} & 1\\\\ \n",
    "            x_{21} & x_{22} & \\dots & x_{2m} & 1\\\\ \n",
    "            \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "            x_{n1} & x_{n2} & \\dots & x_{nm} & 1\\\\ \n",
    "  \\end{matrix} \\right], \\qquad\n",
    "  \\mathbf{w}=\\left[\\begin{matrix}\n",
    "    w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ \\eta\n",
    "  \\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "where $\\bf x_1, \\dots, x_n$ is a vector, $w_1, \\dots, w_n$ are scalars. Therefore our new model is $\\mathbf{y=Xw}$, with $p+1$ unknowns. In order to find the best $\\bf w$, we minimize the difference between the values generated from $\\bf Xw$ and $\\bf y$, as:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\min \\ \\lvert \\lvert \\mathbf{Xw-y} \\rvert \\rvert^2\n",
    "$$\n",
    "\n",
    "to minimize this, we calculate the derivative with respect to each of the weights $\\bf w$ (including intercept):\n",
    "\n",
    "$$\n",
    "\\Delta_w \\mathbf{e} = 2\\mathbf{X^T}(\\mathbf{Xw-y})\n",
    "$$\n",
    "\n",
    "Equating this to 0, and after some equation manipulation we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{X^TXw}=\\mathbf{X^Ty} \\\\\n",
    "\\mathbf{w}=(\\mathbf{X^TX})^{-1}\\mathbf{X^Ty}\n",
    "$$\n",
    "\n",
    "### Task 1.\n",
    "\n",
    "Write a function `least_squares()`, that given an input matrix $\\mathbf{X}_{N,P}$ and output vector $\\mathbf{y}_N$, *directly* calculates and returns the best weight coefficients $\\mathbf{w}_P$. You can use `np.linalg.inv()` to calculate the matrix inverse. Remember to include the bias column to the $X$ matrix. To generate $X$ and $y$, use the function `generate_regression()` provided, you may choose to change some of the optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_regression(n_samples = 100, n_features = 4, n_optimal = 2, bias = 0.0, noise = 0.5, mean_slope = 1.0):\n",
    "    \"\"\"\n",
    "    This function generates an X and y for regression tasks.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    n_samples : int\n",
    "        the number of samples to generate\n",
    "    n_features : int\n",
    "        the number of columns/features\n",
    "    n_optimal : int\n",
    "        the number of useful columns/features, must be <= n_features\n",
    "    bias : double\n",
    "        the bias/intercept\n",
    "    noise : double\n",
    "        variance\n",
    "    mean_slope : double\n",
    "        the approximate slope of optimal values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : matrix (n_samples, n_features)\n",
    "        input matrix\n",
    "    y : vector (n_samples)\n",
    "        output vector\n",
    "    \"\"\"\n",
    "    # create random X matrix\n",
    "    X = np.random.rand(n_samples,n_features)\n",
    "    # good features\n",
    "    w_opt = mean_slope + np.random.rand(n_optimal)*0.1\n",
    "    # append bad features\n",
    "    w = np.hstack((w_opt, np.random.rand(n_features - n_optimal)*0.1))\n",
    "    # add some noise from normal distribution\n",
    "    error = np.random.normal(bias, noise, n_samples)\n",
    "    # apply y = Xw+b+e\n",
    "    y = np.dot(X,w) + bias + error\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "\n",
    "Plot the predicted values $\\hat y$ against the actual values $y$ generated as a scatterplot. These can be estimated using $\\mathbf{\\hat y} \\simeq \\mathbf{Xw}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "\n",
    "Ordinary Least squares is known to suffer from strongly skewed *outliers*. To mitigate this we can apply a regularizing term to the objective minimization function in the form of the $l_2$-norm: $\\lvert \\lvert \\mathbf{w} \\rvert \\vert_2 \\ $:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\min \\ \\lvert \\lvert \\mathbf{Xw-y} \\rvert \\rvert^2 + \\lambda \\lvert \\lvert \\mathbf{w} \\rvert \\vert_2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter to tune the amount of regularization. When derived the optimal minimization of $\\bf w$ is: \n",
    "\n",
    "$$\n",
    "\\mathbf{w}=(\\mathbf{X^TX}+\\lambda I)^{-1}\\mathbf{X^Ty}\n",
    "$$\n",
    "\n",
    "where $I$ refers to the identity matrix.\n",
    "\n",
    "Write a function *ridge()*, with solves the equation *directly* and which has the same parameters as *least_squares()* with an additional parameter $\\lambda=1$ default. Plot $\\hat y$ against $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.\n",
    "\n",
    "Calculate the pearson correlation between $\\hat y$ and $y$. This is calculated as:\n",
    "\n",
    "$$\n",
    "P(x,y)=\\frac{\\sum_{i=1}^{n} (x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar x)^2} \\sqrt{\\sum_{i=1}^n(y_i-\\bar y)^2}}\n",
    "$$\n",
    "\n",
    "where $\\bar x$ and $\\bar y$ refer to the mean of each respective vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.\n",
    "\n",
    "There are cases of where computing the matrix-inverse is intractable, meaning we cannot solve the matrix *directly*. In this case, we can instead take steps in the direction of the *global minimum* through **gradient descent**. The algorithm works as follows:\n",
    "1. Initialise $\\bf w$ at uniform random, $i = 0$\n",
    "1. While i < maximum iterations:\n",
    "    1. Calculate $\\Delta_w \\mathbf{e}$\n",
    "    2. Update $w^{(k+1)}=w^{(k)} - \\gamma \\Delta_w \\mathbf{e}$\n",
    "1. Until convergence\n",
    "\n",
    "where $\\gamma$ is the learning rate.\n",
    "\n",
    "Write a function *gradient_descent()* using the derivative from least-squares, given $X$, $y$, $\\gamma=10^{-3}$ and a number of iterations $K_{max}=10^3$. Save each step and plot $k$ against each weight $w$ (or the mean) to see the minimization in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
