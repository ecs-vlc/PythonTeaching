{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "For many real-world applications, given a set of data $X$ which we consider inputs or measurements observed, and $y$ which is considered an *continuous* output, or useful metric to inter, is there a set of latent coefficients/weights $w$ which when scaled to $X$ can infer $y$? This is the fundamental principle of **linear regression**. In this notation, capital letters $X$ describe *matrices*, and lower-case letters *w,y* describe vectors, with greek letters describing scalar coefficients. Linear models are henceforth:\n",
    "\n",
    "$$\n",
    "\\bf y=w^Tx+\\eta+\\epsilon\n",
    "$$\n",
    "\n",
    "where $w$ is our slope/gradient, $x$ is the input, $\\eta$ is the intercept and $\\epsilon$ is the error.\n",
    "\n",
    "We will however work in matrix notation, and to somplify the math we merge the intercept $\\eta$ into the weights $w$, and add a bias column to $X$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}=\\left[\\begin{matrix}\n",
    "   x_{11} & x_{12} & \\dots & x_{1m} & 1\\\\ \n",
    "            x_{21} & x_{22} & \\dots & x_{2m} & 1\\\\ \n",
    "            \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "            x_{n1} & x_{n2} & \\dots & x_{nm} & 1\\\\ \n",
    "  \\end{matrix} \\right], \\qquad\n",
    "  \\mathbf{w}=\\left[\\begin{matrix}\n",
    "    w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ \\eta\n",
    "  \\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "where $\\bf x_1, \\dots, x_n$ is a vector, $w_1, \\dots, w_n$ are scalars. Therefore our new model is $\\mathbf{y=Xw}$, with $p+1$ unknowns. In order to find the best $\\bf w$, we minimize the difference between the values generated from $\\bf Xw$ and $\\bf y$, as:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\min \\ \\lvert \\lvert \\mathbf{Xw-y} \\rvert \\rvert^2\n",
    "$$\n",
    "\n",
    "to minimize this, we calculate the gradient with respect to each of the weights $\\bf w$ (including intercept):\n",
    "\n",
    "$$\n",
    "\\nabla_w \\mathbf{e} = 2\\mathbf{X^T}(\\mathbf{Xw-y})\n",
    "$$\n",
    "\n",
    "Equating this to 0, and after some equation manipulation we get:\n",
    "\n",
    "$$\n",
    "\\mathbf{X^TXw}=\\mathbf{X^Ty} \\\\\n",
    "\\mathbf{w}=(\\mathbf{X^TX})^{-1}\\mathbf{X^Ty}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.\n",
    "\n",
    "Write a function `least_squares()`, that given an input matrix $\\mathbf{X}_{N,P}$ and output vector $\\mathbf{y}_N$, *directly* calculates and returns the best weight coefficients $\\mathbf{w}_P$. You can use `np.linalg.inv()` to calculate the matrix inverse. Remember to include the bias column to the $X$ matrix. To generate $X$ and $y$, use the function `make_regression()` provided, you may choose to change some of the optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression(n_samples = 100, n_features = 4, n_optimal = 2, bias = 0.0, noise = 0.5, mean_slope = 1.0):\n",
    "    \"\"\"\n",
    "    This function generates an X and y for regression tasks.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    n_samples : int\n",
    "        the number of samples to generate\n",
    "    n_features : int\n",
    "        the number of columns/features\n",
    "    n_optimal : int\n",
    "        the number of useful columns/features, must be <= n_features\n",
    "    bias : double\n",
    "        the bias/intercept\n",
    "    noise : double\n",
    "        variance\n",
    "    mean_slope : double\n",
    "        the approximate slope of optimal values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : matrix (n_samples, n_features)\n",
    "        input matrix\n",
    "    y : vector (n_samples)\n",
    "        output vector\n",
    "    \"\"\"\n",
    "    # create random X matrix\n",
    "    X = np.random.rand(n_samples,n_features)\n",
    "    # good features\n",
    "    w_opt = mean_slope + np.random.rand(n_optimal)*0.1\n",
    "    # append bad features\n",
    "    w = np.hstack((w_opt, np.random.rand(n_features - n_optimal)*0.1))\n",
    "    # add some noise from normal distribution\n",
    "    error = np.random.normal(bias, noise, n_samples)\n",
    "    # apply y = Xw+b+e\n",
    "    y = np.dot(X,w) + bias + error\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "def least_squares(X, y):\n",
    "\tn, p = X.shape\n",
    "\t# add bias\n",
    "\tnX = np.column_stack((np.ones(n), X))\n",
    "\t# solve\n",
    "\treturn np.dot(np.linalg.inv(np.dot(nX.T,nX)),np.dot(nX.T,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "\n",
    "Plot the predicted values $\\hat y$ against the actual values $y$ generated as a scatterplot. These can be estimated using $\\mathbf{\\hat y} \\simeq \\mathbf{Xw}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "def ls_predict(X, w, bias_included=False):\n",
    "    if bias_included:\n",
    "        return np.dot(X,w)\n",
    "    else:\n",
    "        return np.dot(np.column_stack((np.ones(len(X)), X)), w)\n",
    "\n",
    "np.random.seed(5458392)\n",
    "X, y = make_regression()\n",
    "\n",
    "# call method\n",
    "w = least_squares(X, y)\n",
    "# predict yp\n",
    "yp = ls_predict(X, w)\n",
    "\n",
    "plt.scatter(yp,y)\n",
    "plt.plot([-.5, 3.], [-.5, 3.], 'k--')\n",
    "plt.xlabel(\"predicted values\")\n",
    "plt.ylabel(\"actual values\")\n",
    "plt.title(\"Actual against predicted values\")\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "\n",
    "Ordinary Least squares is known to suffer from strongly skewed *outliers*. To mitigate this we can apply a regularizing term to the objective minimization function in the form of the $\\ell_2$-norm: $\\lvert \\lvert \\mathbf{w} \\rvert \\vert_2 \\ $:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\min \\ \\lvert \\lvert \\mathbf{Xw-y} \\rvert \\rvert^2 + \\lambda \\lvert \\lvert \\mathbf{w} \\rvert \\vert_2\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter to tune the amount of regularization. When derived the optimal minimization of $\\bf w$ is: \n",
    "\n",
    "$$\n",
    "\\mathbf{w}=(\\mathbf{X^TX}+\\lambda I)^{-1}\\mathbf{X^Ty}\n",
    "$$\n",
    "\n",
    "where $I$ refers to the identity matrix.\n",
    "\n",
    "Write a function `ridge()`, with solves the equation *directly* and which has the same parameters as `least_squares()` with an additional parameter $\\lambda=1$ default. Plot $\\hat y$ against $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "def ridge(X, y, lamda):\n",
    "\tn, p = X.shape\n",
    "\t# bias\n",
    "\tnX = np.column_stack((np.ones(n), X))\n",
    "\t# solve\n",
    "\treturn np.dot(np.linalg.inv(np.dot(nX.T,nX) + lamda*np.eye(p+1)),np.dot(nX.T,y))\n",
    "\n",
    "w = ridge(X, y, .1)\n",
    "yp = ls_predict(X, w)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(w)\n",
    "\n",
    "plt.scatter(yp, y)\n",
    "plt.plot([-.5, 3.], [-.5, 3.], 'k--')\n",
    "plt.xlabel(\"predicted values\")\n",
    "plt.ylabel(\"actual values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.\n",
    "\n",
    "Calculate the **Pearson correlation** between $\\hat y$ and $y$. This is calculated as:\n",
    "\n",
    "$$\n",
    "P(x,y)=\\frac{\\sum_{i=1}^{n} (x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar x)^2} \\sqrt{\\sum_{i=1}^n(y_i-\\bar y)^2}}\n",
    "$$\n",
    "\n",
    "where $\\bar x$ and $\\bar y$ refer to the mean of each respective vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "def pearson(x, y):\n",
    "    xm = x.mean()\n",
    "    ym = y.mean()\n",
    "    return np.sum((x - xm)*(y - ym)) / (np.sqrt(np.sum((x - xm)**2)) * np.sqrt(np.sum((y - ym)**2)))\n",
    "\n",
    "p = pearson(yp, y)\n",
    "\n",
    "plt.scatter(yp,y,label=\"r={:0.3f}\".format(p))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.\n",
    "\n",
    "There are cases of where computing the matrix-inverse is intractable, meaning we cannot solve the matrix *directly*. In this case, we can instead take steps in the direction of the *global minimum* through **gradient descent**. The algorithm works as follows:\n",
    "1. Initialise $\\bf w$ at uniform random, $i = 0$\n",
    "1. While i < maximum iterations:\n",
    "    1. Calculate $\\Delta_w \\mathbf{e}$\n",
    "    2. Update $w^{(k+1)}=w^{(k)} - \\gamma \\Delta_w \\mathbf{e}$\n",
    "1. Until convergence\n",
    "\n",
    "where $\\gamma$ is the learning rate.\n",
    "\n",
    "Write a function `gradient_descent()` using the derivative from least-squares, given $X$, $y$, $\\gamma=10^{-3}$ and a number of iterations $K_{max}=10^3$. Save each step and plot $k$ against each weight $w$ (or the mean) to see the minimization in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your codes here\n",
    "def gradient_descent(X, y, gamma = .001, n_iter=500):\n",
    "\tn, P = X.shape\n",
    "\tnX = np.column_stack(((np.ones(n,)), X))\n",
    "\tsaved_w = np.empty((P+1, n_iter))\n",
    "\tw = np.random.rand(P+1)\n",
    "\tsaved_w[:,0] = w\n",
    "\tfor i in range(1,n_iter):\n",
    "\t\tdE = np.dot((2*nX.T),(np.dot(nX,w) - y))\n",
    "\t\tw -= gamma*dE\n",
    "\t\tsaved_w[:,i] = w\n",
    "\treturn saved_w, w\n",
    "\n",
    "N_iter = 500\n",
    "\n",
    "sw, w = gradient_descent(X, y, .001, N_iter)\n",
    "\n",
    "print(X.shape, y.shape, sw.shape, w.shape)\n",
    "\n",
    "t = np.arange(N_iter)\n",
    "for i in range(len(w)):\n",
    "\tplt.plot(t,sw[i,:],'--',label=i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
