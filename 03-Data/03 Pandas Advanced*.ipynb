{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas Analysis\n",
    "\n",
    "Here we will touch on some of the most complex areas of Pandas, continuing from a number of the Intermediate topics mentioned previously, to give you as full of an experience as possible using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas.melt`: From wide to long\n",
    "\n",
    "A number of the packages require data to exist in *long-form*, this often means that columns contain duplicates and is memory and disk intensive. It is far more common to keep data in wide-form. However when we need to convert data that has many similar-like columns into *long-form*, `pd.melt` is one of the best functions in Pandas to achieve this.\n",
    "\n",
    "Take the `cdystonia` dataset for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia = pd.read_csv(\"datasets/cdystonia.csv\")\n",
    "print(cdystonia.shape)\n",
    "cdystonia.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using aforementioned methods, we can expand out the `twstrs` response column to be multiple columns using a *pivot*. Here we use the `week` as the columns (identical to observation `obs`), and use the set difference to eliminate, keeping all the other columns available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia_wide = cdystonia.pivot_table(\"twstrs\", index=cdystonia.columns.difference([\"twstrs\",\"obs\",\"week\"]).tolist(), columns=\"week\")\n",
    "print(cdystonia_wide.shape)\n",
    "cdystonia_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that $(631,9)$ is substantially larger than $(109,6)$ in terms of dimensional size. By specifying the columns we want to keep as identifiers, `pd.melt` selects every other column and collapses it into a single column, that we name back as `twstrs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia_long = pd.melt(cdystonia_wide.reset_index(), id_vars=[\"age\",\"id\",\"patient\",\"sex\",\"site\",\"treat\"], value_name=\"twstrs\", var_name=\"week\")\n",
    "cdystonia_long.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized String Operations\n",
    "\n",
    "One strength of Python is its relative ease in handling and manipulating string data. Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when working with (read: cleaning up) real-world data. In this section, we'll walk through some of the Pandas string operations, and then take a look at using them to partially clean up a very messy dataset of recipes collected from the Internet.\n",
    "\n",
    "If we recall from NumPy, one of the key advantages was the *vectorization* of mathematical operations, such as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([2,3,5,7,11,13])\n",
    "x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas for arrays of strings, NumPy does not provide such simple access, and we have to fall back to using a Pythonic list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(['peter','Paul','mary','guido'])\n",
    "[s.capitalize() for s in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, this Pythonic method will break in cases where there is missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(['peter','Paul',None,'mary','guido'])\n",
    "[s.capitalize() for s in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the `str` attribute of `pd.Series` and `pd.Index` objects containing string information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.Series([\"Jeff\", \"alan\", \"Steve\", \"gUIDO\", None, \"job\", None])\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call a single method to capitalize the entries, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available methods in `pandas.str`\n",
    "\n",
    "Nearly all of the Python built-in string methods are mirrored in Pandas vectorized string methods, here is a tabular list:\n",
    "\n",
    "| & | & | &  | &|\n",
    "|------- | ----------- | ----------- | ------------- |\n",
    "| `len()` | `lower()` | `translate()` | `islower()` |\n",
    "| `ljust()` | `rjust()` | `lower()` | `upper()` | \n",
    "| `startswith()` | `endswith()` | `find()` | `isnumeric()` |\n",
    "| `center()` | `rfind()` | `isalnum()` | `isdecimal()` | \n",
    "| `zfill()` | `index()` | `isalpha()` | `split()` |\n",
    "| `strip()` | `rindex()` | `isdigit()` | `rsplit()` |\n",
    "| `rstrip()` | `capitalize()` | `isspace()` | `partition()` |\n",
    "| `lstrip()` | `swapcase()` | `istitle()` | `rpartition()` |\n",
    "\n",
    "Note that there are variable return values, for instance `lower()` returns a string, but `len()` returns an integer, `startswith()` returns a boolean value, etc.\n",
    "\n",
    "### Additional method using regular expressions\n",
    "\n",
    "This is where the true power of Pandas comes in: not only can we do direct matching and string manipulation, but also provide functionality to examine the content of each element using a regular expression. Some of the below functions we can use are:\n",
    "\n",
    "| **Method** | **Description** |\n",
    "| ---------- | -------------------------------- |\n",
    "| `match()` | Calls `re.match()` on each element, returning a boolean |\n",
    "| `extract()` | Calls `re.extract()` on each element, returning matched groups as strings |\n",
    "| `findall()` | Calls `re.findall()` on each element |\n",
    "| `replace()` | Replaces occurences of pattern with some other string |\n",
    "| `contains()` | Calls `re.search()` on each element, returning a boolean |\n",
    "| `count()` | Count occurrences of pattern |\n",
    "| `split()` | Calls `str.split()`, but accepts regular expressions |\n",
    "| `rsplit()` | Calls `str.rsplit()` but accepts regular expressions |\n",
    "\n",
    "With these, we have a wide range of interesting operations. For example, we can extract the first name from each by asking for a contiguous group of characters at the beginning of the element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\n",
    "                   'Eric Idle', 'Terry Jones', 'Michael Palin'], name=\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.extract(\"([A-Za-z]+)\", expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we return `expand=True`, we return a 1-D dataframe, else we get a `pd.Series`. Or we could do something more complicated, like finding all the names that start and end with a consonant, make use of the start-of-string (^) and end-of-string (\\$) regular expression characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.findall(r\"^[^AEIOU].*[^aeiou]$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscallaneous methods\n",
    "\n",
    "Finally, there are a number of convenient operations which Pandas uniquely provides that can be invaluable when *function chaining*:\n",
    "\n",
    "| **Method** | **Description** |\n",
    "| ----------- | ----------------------------- |\n",
    "| `get()` | Index each element |\n",
    "| `slice()` | Slice each element |\n",
    "| `slice_replace()` | Replace slice in each element with passed value |\n",
    "| `cat()` | Concatenate strings |\n",
    "| `repeat()` | Repeat values |\n",
    "| `normalize()` | Return a unicode form of the string |\n",
    "| `pad()` | Add whitespace to the left, right or both sides of a string |\n",
    "| `wrap()` | Split long strings into lines of length less than a given width |\n",
    "| `join()` | Join strings in each element of the Series with passed separator |\n",
    "| `get_dummies()` | Extract dummy variables as DataFrame |\n",
    "\n",
    "### Vectorized item access and slicing\n",
    "\n",
    "The `get()` and `slice()` operations, enable vectorized element access from each array. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.slice(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte.str.split(\" \", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator Variables\n",
    "\n",
    "Another method that requires a bit of extra explanation is the `get_dummies()` method. This is useful when your data has a column containing some sort of coded indicator. For example, we might have a dataset that contains information in the form of codes, such as A=\"born in America,\" B=\"born in the United Kingdom,\" C=\"likes cheese,\" D=\"likes spam\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=pd.Series([\"B|C|D\",\"B|D\",\"A|C\",\"B|D\",\"B|C\", \"B|C|D\"])\n",
    "info.name=\"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_monte = pd.concat([monte, info],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_dummies` routine lets you split out indicator variables into a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_monte[\"info\"].str.get_dummies(\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Types\n",
    "\n",
    "Categoricals are a pandas data type corresponding to categorical variables, such as from statistics. A categorical variable takes on a limited, fixed, number of possible values. Examples include gender, blood type, country or rating. Categorical data may be ordered, but numerical operations are not possible on them.\n",
    "\n",
    "All of the values in categorical data are either in categories or `np.nan`. Order is defined by the order of *categories*, not the lexical order of the values. Using a categorical data type has a number of **advantages**:\n",
    "\n",
    "- A string variable consisting of only a few different values can be *efficiently* stored internally as each string is represented by an integer, and only unique strings are in the categories array.\n",
    "- Sorting through an ordered categorical variable is substantially faster.\n",
    "- Provides valuable metadata to Pandas when it comes to smart plotting, operations, etc.\n",
    "\n",
    "Much of this material is drawn from the Pandas documentation, which is extensive and found [here](https://pandas.pydata.org/pandas-docs/stable/categorical.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.Categorical(['a', 'b', 'b', 'c', 'a', 'b', 'a', 'a', 'a', 'c'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can provide information as to the ordering of the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting an existing 'object' feature into a category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([\"air\", \"water\", \"fire\", \"fire\", \"water\", \"earth\", \"fire\", \"fire\", \"water\", \"air\"])\n",
    "s.astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Data\n",
    "\n",
    "Pandas as a tool was initially developed in the context of financial modelling, so as you might expect, there is a rather large suite of tools for working with dates, times and time-indexed data. There are a number of different formats that date data can come in:\n",
    "\n",
    "- *Time stamps* reference particular moments in time (e.g Dec 25, 2011 at 7:45pm).\n",
    "- *Time intervals* and periods reference a length of time with a beginning and end point.\n",
    "- *Time deltas* or durations reference an exact length of time (e.g duration of 22.56 seconds).\n",
    "\n",
    "### Native Datetime\n",
    "\n",
    "Natively, Python has a representation of datetime objects from the `datetime` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime(year=2015, month=7, day=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the `dateutil` parse module to use dates from a variety of formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "parser.parse(\"4th of July, 2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse(\"4th of July, 2015\").strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final line, we've used one of the standard string format codes for printing dates `%A`, where this is more to read within the documentation of Python's datetime function. \n",
    "\n",
    "### NumPy's `datetime64`\n",
    "\n",
    "NumPy introduces a native time-series data type which is encoded as a 64-bit integer, and allows arrays of dates to be represented very compactly. The `datetime64` has a specific input format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.array(\"2015-07-04\", dtype=np.datetime64)\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this date formatted, we can quickly do vectorized operations on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + np.arange(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the uniform type of NumPy `datetime64` arrays, this type of operation can be accomplished more quickly than if we were working with native Python objects. One of the important features of `datetime64` and `timedelta64` is that they are built on a fundamental time unit. This means that because the object is limited to 64-bit precision, the range of encodable times is $2^{64}$ times this fundamental unit. This means that `datetime64` imposes a trade-off between *time resolution* and *maximum time span*.\n",
    "\n",
    "For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of $2^{64}$ nanoseconds, or 600 years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64(\"2015-07-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.datetime64('2015-07-04 12:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table (from the NumPy `datetime64` documentation) lists the available format codes along with the relative timespans they can encode:\n",
    "\n",
    "| **Code** | **Meaning** | **Time span (relative)** |\n",
    "| --- | ---------- | ----------------- |\n",
    "| Y | Year | $\\pm 9.2 \\times 10^{18}$ years | \n",
    "| M | Month | $\\pm 7.6 \\times 10^{17}$ years |\n",
    "| W | Week | $\\pm 1.7 \\times 10^{17}$ years |\n",
    "| D | Day | $\\pm 2.5 \\times 10^{16}$ years |\n",
    "| h | Hour | $\\pm 1 \\times 10^{15}$ years |\n",
    "| m | Minute | $\\pm 1.7 \\times 10^{13}$ years |\n",
    "| s | Second | $\\pm 2.9 \\times 10^{12}$ years |\n",
    "| ms | Millisecond | $\\pm 2.9 \\times 10^{9}$ years |\n",
    "| $\\mu$s | Microsecond | $\\pm 2.9 \\times 10^{6}$ years |\n",
    "| ns | Nanosecond | $\\pm 292$ years |\n",
    "| ps | Picosecond | $\\pm 106$ days |\n",
    "\n",
    "For the real world, `datetime64[ns]` is sufficiently precise, with 292 years usually being more than sufficient for most modern applications.\n",
    "\n",
    "### Dates and times in Pandas\n",
    "\n",
    "Pandas builds on top of NumPy and the native Python libraries to get the best-of-both-worlds effect; efficient storage and vectorized interface with ease-of-use. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.to_datetime(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date.strftime(\"%A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + pd.to_timedelta(np.arange(12),\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Series Indexing\n",
    "\n",
    "Where the Pandas time-series tools really become useful is when you can *index data by timestamps*. For example we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',\n",
    "                          '2015-07-04', '2015-08-04'])\n",
    "data = pd.Series([0, 1, 4, 2], index=index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in a `Series`, we can make use of the `Series` indexing patterns previously uncovered, passing values that can be interpreted as dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"2014-07-04\":\"2015-07-04\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional date-only indexing operations, such as passing a year or a month to obtain a slice of all data from that year/month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"2015\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series Data Structures\n",
    "\n",
    "Pandas uses a number of fundamental data structures when describing time-series data:\n",
    "\n",
    "| **Structure** | **Description** |\n",
    "| ------------- | ---------------------------------------- |\n",
    "| Time-stamp | Basic `Timestamp` type, a replacement for Python's native `datetime`, <br> but more efficient from NumPy. Associated to `DatetimeIndex` object. |\n",
    "| Time Periods | `pd.Period` object provided. Encodes a fixed-frequency <br> interval based on `np.datetime64`. Associated to `PeriodIndex` object. |\n",
    "| Time Delta/Duration | Pandas provides `Timedelta` type. More efficient than native <br> Python `datetime.timedelta` object, and based on `np.timedelta64`. <br> Associated to `TimedeltaIndex` object. | \n",
    "\n",
    "The fundamental objects are the `Timestamp` and `DatetimeIndex` objects. While these objects can be created directly, it's more popular to use the `pd.to_datetime()` function, which can parse a wide range of formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime([datetime(2015, 7, 3), \"4th of July, 2015\", \"2015-Jul-6\", \"07-07-2015\", \"20150708\"])\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any `DatetimeIndex` can be converted to a `PeriodIndex` with the `to_period()` function with the addition of a frequency code; in this case a daily frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.to_period(\"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TimedeltaIndex` is created, for instance, when a date is subtracted from another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular sequences: `pd.date_range()`\n",
    "\n",
    "To aid in the creation of date sequences, Pandas has a number of functions that make input far easier:\n",
    "\n",
    "| **Function** | **Description** |\n",
    "| ------------ | ----------------------------- |\n",
    "| `pd.date_range()` | Creates a sequence of time-stamps |\n",
    "| `pd.period_range()` | Create a sequence of time periods |\n",
    "| `pd.timedelta_range()` | Creates a sequence of time-deltas |\n",
    "\n",
    "This format follows similarly to NumPy's `arange()` and `linspace()` functions, which accept a start point, endpoint, and optional stepsize into a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(\"2015-07-03\",\"2015-07-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(\"2015-07-03\", periods=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacing can be modified by altering the `freq` argument, which defaults to days (D). For example, we could step in hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(\"2015-07-03\", periods=8, freq=\"H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies and Offsets\n",
    "\n",
    "Pandas provides a number of additional codes to NumPy's standard when defining frequency/date offsets:\n",
    "\n",
    "| **Code** | **Description** |  **Code** | **Description** |\n",
    "| --- | -------------------- | --- | --------------------- |\n",
    "| D | Calendar day | H | Hours |\n",
    "| W | Weekly | A | Year end |\n",
    "| M | Month end | M | Minutes | \n",
    "| Q | Quarter end | S | Seconds |\n",
    "| B | Business day | L | Milleseconds |\n",
    "\n",
    "Additionally, we can combine codes with numbers to specify more unique frequencies. For example, a frequency of 2 hours and 30 minutes, we can combine hour (H) and minute (T) codes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling, Shifting and Windowing\n",
    "\n",
    "The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools. The benefits of indexed data in general (automatic alignment during operations, intuitive data slicing and access, etc.) still apply, and Pandas provides several additional time series-specific operations.\n",
    "\n",
    "For these examples, we'll look at some stock price data from Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog = pd.read_csv(\"datasets/goog_stock.csv\", index_col=0, \n",
    "                   parse_dates=True, skipinitialspace=True).sort_index()\n",
    "goog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog[\"Close\"].plot(grid=True)\n",
    "plt.ylabel(\"Closing Price\")\n",
    "plt.title(\"GOOG STOCK\")\n",
    "plt.savefig(\"images/goog_stock.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common needs for time-series data is resampling at a higher or lower frequency. This can be done using the `resample()` method, or using `asfreq()`. The primary difference is that, while `resample()` is fundamentally a data aggregation technique, `asfreq()` is a data selection method. \n",
    "\n",
    "For the google data, let's compare what both return when we down-sample the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog[\"Close\"].plot(alpha=.5, style=\"-\")\n",
    "goog[\"Close\"].resample(\"BQ\").mean().plot(style=\":\")\n",
    "goog[\"Close\"].asfreq(\"BQ\").plot(style=\"--\")\n",
    "plt.legend([\"input\", \"resample\", \"asfreq\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `resample` reports the average of the previous year, whereas `asfreq` reports the value at the end of the year. \n",
    "\n",
    "For up-sampling, `resample()` and `asfreq()` are largely equivalent, although `resample()` has considerably more options. The default for both methods is to leave the up-sampled points empty, i.e NA values. We also have options to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True, figsize=(6,6))\n",
    "ss = goog[\"Close\"].iloc[:20]\n",
    "ss.asfreq(\"D\").plot(ax=ax[0], marker=\"o\")\n",
    "ss.asfreq(\"D\", method=\"bfill\").plot(ax=ax[1], style=\"-o\")\n",
    "ss.asfreq(\"D\", method=\"ffill\").plot(ax=ax[1], style=\"--o\")\n",
    "ax[1].legend([\"back-fill\", \"forward-fill\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top panel is the default `asfreq` behaviour, non-business days are left as NA values. The bottom panel shows the differences between *forward-filling* and *backward-filling*.\n",
    "\n",
    "### Time-shifts\n",
    "\n",
    "Another common time-series specific operation is shifting of data in time. Pandas has two methods for this operation:\n",
    "\n",
    "1. `shift()`: Shifts the data by a given frequency.\n",
    "2. `tshift()`: Shifts the index by a given frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, sharey=True, figsize=(6,8))\n",
    "\n",
    "# apply a frequency to the data\n",
    "goog_d = goog[\"Close\"].asfreq('D', method='pad')\n",
    "\n",
    "goog_d.plot(ax=ax[0])\n",
    "goog_d.shift(900).plot(ax=ax[1])\n",
    "goog_d.tshift(900).plot(ax=ax[2])\n",
    "\n",
    "# legends and annotations\n",
    "local_max = pd.to_datetime(goog_d.index[30])\n",
    "offset = pd.Timedelta(900, 'D')\n",
    "\n",
    "ax[0].legend(['input'], loc=2)\n",
    "ax[0].get_xticklabels()[2].set(weight='heavy', color='red')\n",
    "ax[0].axvline(local_max, alpha=0.3, color='red')\n",
    "\n",
    "ax[1].legend(['shift(900)'], loc=2)\n",
    "ax[1].get_xticklabels()[2].set(weight='heavy', color='red')\n",
    "ax[1].axvline(local_max + offset, alpha=0.3, color='red')\n",
    "\n",
    "ax[2].legend(['tshift(900)'], loc=2)\n",
    "ax[2].get_xticklabels()[1].set(weight='heavy', color='red')\n",
    "ax[2].axvline(local_max + offset, alpha=0.3, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that `shift(900)` shifts the data by 900 days, pushing some of it off the end of the graph, while `tshift(900)` shifts the index values by 900 days. \n",
    "\n",
    "A common context for this type of shift is in computing differences over time. For example, we use shifted values to compute the one-year return on investment for Google stock over the course of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = 100 * (goog[\"Close\"].tshift(-365) / goog[\"Close\"] - 1)\n",
    "ROI.plot()\n",
    "plt.ylabel(\"Return on Investment (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us to see the overall trend in this particular stock option.\n",
    "\n",
    "### Rolling windows\n",
    "\n",
    "Rolling statistics are another type of series-specific operation implemented in Pandas. This is achieved through the `rolling()` attribute of `Series` and `DataFrame` objects, which returns a view similar to the `groupby()` operation. This rolling view makes available a number of aggregation operations by default.\n",
    "\n",
    "For example, here is the one-year rolling mean and standard deviation of Google stocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = goog[\"Close\"].rolling(365//2, center=True)\n",
    "data = pd.DataFrame({\"input\": goog[\"Close\"],\n",
    "                     \"half-year-mean\": rolling.mean(),\n",
    "                     \"half-year-std\": rolling.std()})\n",
    "ax = data.plot(style=[\"-\",\"--\",\":\"])\n",
    "ax.lines[0].set_alpha(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `groupby` operations, `aggregate()` and `apply()` methods can be used for custom rolling computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Chaining\n",
    "\n",
    "You may have noticed previously that a number of the operations can be chained together, skipping intermediate states of the `DataFrame`. This is known as **method chaining** and really helps not only with performance but *code readability*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cdystonia.assign(age_group=pd.cut(cdystonia.age, [0, 30, 40, 50, 60, 70, 80, 90], right=False))\n",
    "    .groupby(['age_group','sex']).mean()\n",
    "    .twstrs.unstack(\"sex\")\n",
    "    .fillna(0.0)\n",
    "    .plot.barh(figsize=(10,5)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipes\n",
    "\n",
    "One of the problems with method chaining is that it requires all of the functionality you need for data processing to be implemented somewhere as methods which return the actual DataFrame object in order to chain. Occasionally we want to do custom manipulations to our data, this is solved in *pipe*.\n",
    "\n",
    "For example, we may wish to calculate the *proportion of twstrs* in the whole dataset to see differences between each patient in proportional terms across time to all of the other patients in their age group, their state of pain etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_proportions(df, axis=1):\n",
    "    row_totals = df.sum(axis)\n",
    "    return df.div(row_totals, True - axis)\n",
    "\n",
    "(cdystonia.assign(age_group=pd.cut(cdystonia.age, [0, 30, 40, 50, 60, 70, 80, 90], right=False))\n",
    "    .groupby([\"week\",\"age_group\"]).mean()\n",
    "    .twstrs.unstack(\"age_group\")\n",
    "    .pipe(to_proportions, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the proportion of response variable across the age groups, per week.\n",
    "\n",
    "## Data Transformation\n",
    "\n",
    "We have several options for *transforming* labels and other columns into more useful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia.treat.replace({'Placebo': 0, \"5000U\": 1, \"10000U\": 2}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdystonia.treat.astype(\"category\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(cdystonia.age, [20,40,60,80], labels=[\"Young\",\"Middle-Aged\",\"Old\"])[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use qcut to automatically divide our data into even-sized $q$-tiles. For example $q=4$ refers to quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(cdystonia.age, 4)[-8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Dataframes\n",
    "\n",
    "*Sparse* version of Series and DataFrame are implemented in Pandas. They are not sparse in the typical sense, rather these objects are **compressed** where any data matching a specific value (`NaN`/missing) is omitted. A special `SparseIndex` object tracks where data has been *sparsified*. See this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(np.random.randn(10))\n",
    "ts[2:-2] = np.nan\n",
    "sts = ts.to_sparse()\n",
    "sts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to_sparse()` method allows us to fill the value with something other than `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.fillna(0.).to_sparse(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Sparse objects are mostly useful for memory-efficient reasons. Suppose you had a mostly `NaN` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(100,100))\n",
    "df_sp = df.where(df < 0.02).to_sparse()\n",
    "print(df_sp.density)\n",
    "df_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memory usage [sparse]: %d bytes\\nMemory usage [dense]: %d bytes\" % (df_sp.memory_usage().sum(), df.memory_usage().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas also supports creating sparse dataframes directly from `scipy.sparse` matrices. It is worth mentioning that Pandas converts scipy matrices NOT in COOrdinate format to COO, copying data as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "scip_sps = sparse.coo_matrix(np.random.choice([0,1], size=(1000,1000), p=(.95, .05)))\n",
    "scip_sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.SparseDataFrame(scip_sps)\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Performance Pandas: `eval()` and `query()`\n",
    "\n",
    "As we've seen in previous sections, the power of the PyData stack is built upon the capacity of NumPy and Pandas to push basic operations into C via an intuitive syntax. Examples are vectorized/broadcasted operations in NumPy, and grouping-type operations in Pandas. Whilst these abstractions are efficient for common use cases, they often rely on the creation of temporary intermediate objects, which can cause undue overhead in computational time and memory use.\n",
    "\n",
    "Recently (version 0.13+), Pandas includes some experimental tools that allow you to directly access C-speed operations without costly allocation of intermediate arrays. These are the `eval()` and `query()` functions, which rely on the [`numexpr` package](https://github.com/pydata/numexpr).\n",
    "\n",
    "### Motivating `query()` and `eval()`: Compound Expressions\n",
    "\n",
    "As we've seen before, NumPy and Pandas support fast vectorized operations; for example adding elements of 2 arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(777)\n",
    "x = rng.rand(1000000)\n",
    "y = rng.rand(1000000)\n",
    "%timeit x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously, this is much faster than doing the addition via a Python loop or comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this abstraction is less efficient when computing compound expressions. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (x > .5) & (y < .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because NumPy evaluates each subexpression, every intermediate step is explicitly allocated in memory. If $x$ and $y$ are very large, this can lead to significant memory and computational overhead. The `numexpr` library gives you the ability to compute this type of compound expression element by element, without the need for full allocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_numexpr = numexpr.evaluate(\"(x > 0.5) & (y < 0.5)\")\n",
    "np.allclose(mask, mask_numexpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pd.eval()`\n",
    "\n",
    "The `eval()` function in Pandas uses string expressions to efficiently compute operations using `DataFrame` objects. For example, consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = 100000, 100\n",
    "rng = np.random.RandomState(777)\n",
    "df1,df2,df3,df4 = (pd.DataFrame(rng.rand(nrow,ncol)) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the sum of all 4 DataFrames using a typical approach, we would:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df1 + df2 + df3 + df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same result can be computed via `pd.eval` by constructing as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pd.eval(\"df1 + df2 + df3 + df4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `eval()` version is about 50% faster, while giving the same result.\n",
    "\n",
    "### Operations supported by `pd.eval()`\n",
    "\n",
    "| **Operation Type** | **Description** | **Code Example** |\n",
    "| ------------- | ------------------------------ | ------------------- |\n",
    "| Arithmetic | `pd.eval` supports all arithmetic operators. | `result = pd.eval('-df1 * df2 / (df3 + df4) - df5')` |\n",
    "| Comparison | `pd.eval` supports all comparison operators. | `result = pd.eval('df1 < df2 <= df3 != df4')` |\n",
    "| Bitwise | `pd.eval` supports the `&`, `and`, `or` and \\| bitwise operators | `result = pd.eval('(df1 < 0.5) & (df2 < 0.5) or (df3 < df4')` |\n",
    "| Object attributes and indices | `pd.eval` supports access to object attributes <br> via the `obj.attr` syntax, and index via the `obj[index]` syntax. | `result = pd.eval('df2.T[0] + df3.iloc[1]')` |\n",
    "\n",
    "Other operations such as function calls, conditional statements, loops and other more involved constructs are currently *not* implemented in `pd.eval()`. Some of these may exist in the `numexpr` library itself.\n",
    "\n",
    "### `DataFrame.eval()` for column-wise operations\n",
    "\n",
    "Pandas has a top-level `pd.eval()` function, DataFrames have an `eval()` method that works similarly. The benefit of `eval()` is that columns can be referred to *by name*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rng.rand(1000,3), columns=[\"A\",\"B\",\"C\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = (df[\"A\"] + df[\"B\"]) / (df[\"C\"] - 1)\n",
    "R = pd.eval(\"(df.A + df.B) / (df.C - 1)\")\n",
    "np.allclose(res1,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = df.eval(\"(A + B) / (C - 1)\")\n",
    "np.allclose(res1,R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment in `df.eval()`\n",
    "\n",
    "In addition to the previous operations, `DataFrame.eval()` allows assignment to any column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.eval(\"D = (A + B) / C\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also allows modification of existing columns.\n",
    "\n",
    "### Local variables in `df.eval()`\n",
    "\n",
    "`df.eval()` also supports additional syntax that lets it work with local Python variables. Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mean = df.mean(1)\n",
    "res2 = df[\"A\"] + col_mean\n",
    "R3 = df.eval(\"A + @col_mean\")\n",
    "np.allclose(res2,R3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataFrame.query()` method\n",
    "\n",
    "As for the examples used for `df.eval()`, this is an expression involving columns in the DataFrame. However this is a type of **filtering** operation instead of evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = df[(df.A < 0.5) & (df.B < 0.5)]\n",
    "R4 = df.query(\"A < 0.5 and B < 0.5\")\n",
    "np.allclose(res3,R4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being a more efficient computation, this is much easier to read and understand. Note that the `query()` method also accepts the `@` flag to mark local variables.\n",
    "\n",
    "### When to use Performance functions\n",
    "\n",
    "When considering whether to even bother using these functions, there are two main considerations:\n",
    "\n",
    "1. *Computation time*\n",
    "2. *Memory use*\n",
    "\n",
    "Memory use is the most predictable aspect. As already mentioned, every compound expression involving NumPy arrays or Pandas `DataFrames` will result in implicit creation of temporary arrays. If the size of the temporary DataFrame is significant compared to available system memory, then it's a good idea to use an `eval()` or `query()` expression. You can check the approximate size of your array in bytes using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the performance side, `eval()` is faster even when you are not maxing our your memory. The main bottleneck is usually how your temporary DataFrame size compares to the size of your L1 or L2 CPU cache on your system. It is often that there is not a considerable difference in computation times between the traditional methods and eval methods, but a larger dividend in saved memory and cleaner syntax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
