{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Time-Series\n",
    "\n",
    "Working with time-series data often has some unique graphs. It made sense to break this into a small section by itself to explore the interactions with time-series.\n",
    "\n",
    "In particular, time-series data can sometimes be the most difficult to work with, and fluency with handling date objects will come in handy.\n",
    "\n",
    "There are a number of objects which are primarily found in **NumPy** that concern time-series:\n",
    "- DateTimeIndex\n",
    "- PeriodIndex\n",
    "- TimeDeltaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas.plotting as pplt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with some cumulative randomly distributed values to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "np.random.seed(19680801)\n",
    "x = pd.DataFrame(np.random.randn(N,3).cumsum(0), index=pd.date_range(\"1/1/2010\", periods=N, freq=\"H\"))\n",
    "x.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth mentioning that default Matplotlib is absolutely awful at plotting the x axis (i.e using **plt.plot**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.plot(subplots=True, figsize=(8,8), color=['r','g','b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Deltas\n",
    "\n",
    "Often we may be interested in data which *changes* with respect to time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(np.random.randn(200), index=pd.to_timedelta(np.arange(200), \"H\"))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain a delta by subtracting together two timeseries datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(\"1/1/2015\", periods=20) - pd.date_range(\"1/6/2015\", periods=20, freq=\"H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Frequencies and Offsets\n",
    "\n",
    "Fundamental to these Pandas time series tools is the concept of a frequency or date offset. We can use such codes to specify any desired frequency spacing:\n",
    "\n",
    "- **D**: Calendar Day\n",
    "- **W**: weekly\n",
    "- **M**: month end\n",
    "- **Q**: quarter end\n",
    "- **A**: year end\n",
    "- **H**: hours\n",
    "- **T**: minutes\n",
    "- **S**: seconds\n",
    "- **L**: milliseconds\n",
    "- **U**: microseconds\n",
    "- **N**: nanoseconds\n",
    "- **B**: business day\n",
    "- **BM**: business month end\n",
    "- **BQ**: business quarter\n",
    "- **BA**: business year end\n",
    "- **BH**: business hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"10S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(0, periods=9, freq=\"3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "\n",
    "In time-series analysis we could often be handling data with a small number of *missing values*. Assuming the frequency of our time-series is appropriately small, we could interpolate missing values using a *linear* or *quadratic* interpolator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(153446)\n",
    "N=500\n",
    "z = pd.Series(np.random.randn(N).cumsum(), index=pd.date_range(\"4/1/2018\", periods=N, freq=\"T\"))\n",
    "zn = z.where(np.random.choice([0,1], len(z), p=[.1, .9]) == 1)\n",
    "print(zn.count(), zn.interpolate().count())\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2, figsize=(12,6))\n",
    "z.plot(ax=ax[0], label=\"original\")\n",
    "zn.plot(ax=ax[1], label=\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a linear or other interpolator, this data shouldn't be too far from it's original, we can check this by subtracting the interpolated value from the real exact value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn_int = [zn.interpolate(method='linear'), zn.interpolate(method='quadratic'), zn.interpolate(method='nearest'),\n",
    "         zn.interpolate(method='cubic')]\n",
    "\n",
    "fig,ax=plt.subplots(ncols=4, nrows=2, figsize=(16,6))\n",
    "\n",
    "for i,interp in enumerate(zn_int):\n",
    "    interp.plot(ax=ax[0,i])\n",
    "    interp.sub(z).plot(ax=ax[1,i])\n",
    "    print(\"i={}, MSE={:0.4f}\".format(i, interp.sub(z).apply(np.square).mean()))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling/Converting\n",
    "\n",
    "We've looked at converting frequencies previously as part of the Pandas learning: let's put this to use in some plots:\n",
    "\n",
    "* **Resampling**: fundamentally *aggregates* the data.\n",
    "* **Converting**: fundamentally *selects* data points.\n",
    "\n",
    "Below we resample and convert from a more-to-less frequent sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.plot(alpha=.7, style=\"-\", label=\"input\")\n",
    "z.resample(\"H\").mean().plot(color='b', style=\":\", label=\"resample\")\n",
    "z.asfreq(\"H\").plot(style=\"--\", color='r', label=\"asfreq\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reverse case, we interpolate missing points from a less-to-more frequent sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(2, figsize=(8,6), sharex=True)\n",
    "z[:4].asfreq(\"S\").plot(marker='o', linestyle='-', ax=ax[0])\n",
    "z[:4].asfreq(\"S\", method='bfill').plot(ax=ax[1], style='o-', c='k')\n",
    "z[:4].asfreq(\"S\", method='ffill').plot(ax=ax[1], style='o--', c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity\n",
    "\n",
    "A timeseries is said to be **stationary** if its statistical properties such as mean and variance remain *constant* over time. This has important implications to timeseries models when it comes to *forecasting*.\n",
    "\n",
    "Here is an example of a stationary time-series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0,20*np.pi,500)\n",
    "y = np.sin(a)\n",
    "plt.plot(a,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationarity is defined as:\n",
    "- constant mean\n",
    "- constant variance\n",
    "- an autocovariance that does not depend on time\n",
    "\n",
    "We can test for stationarity through visual inspection or via the *Dickey-Fuller Test*.\n",
    "\n",
    "Let's check this using the popular Air Passengers dataset for a flight every month over 12 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger = pd.read_csv(\"AirPassengers.csv\", index_col=0, parse_dates=True)\n",
    "print(passenger.dtypes)\n",
    "passenger.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see there is an **overall increasing trend** in the data along with some seasonal variations. We want to *make* the time series stationary, by performing various **transformations** to the data. There are two concepts involved in making data non-stationary:\n",
    "- Trend; varying mean over time.\n",
    "- Seasonality; variations at specific time-frames. e.g people may prefer to book a plane flight in the summer months. \n",
    "\n",
    "Knowing of these factors, we can attempt to eliminate them from the series to get a returned stationary series. Then statistical forecasting techniques could be implemented on the series. \n",
    "\n",
    "### Estimating & Eliminating Trend\n",
    "\n",
    "One of the tricks we can use is *transformation*, for instance to positive trends we could apply a penalizing term to higher values by taking the $\\log$ or square-root.\n",
    "\n",
    "Here we can see a marked reduction in the variance, but we could use one of the previous methods to remove trends, such as *moving averages* (MA), *smoothing* and *polynomial fitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psng_log = passenger.apply(np.log)\n",
    "psng_ma = psng_log.rolling(12, center=True).mean()\n",
    "psng_mstd = psng_log.rolling(12, center=True).std()\n",
    "psng_ma_diff = psng_log - psng_ma\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2, figsize=(12,6))\n",
    "psng_log.plot(ax=ax[0], label=\"input\", legend=False)\n",
    "psng_ma.plot(ax=ax[0], color='r', label=\"rolling 12\", legend=False)\n",
    "psng_ma_diff.plot(ax=ax[1], color='g', legend=False)\n",
    "psng_ma_diff.rolling(12, center=True).agg(['mean','std']).plot(ax=ax[1], legend=False)\n",
    "\n",
    "ax[0].set_title(\"log_series with rolling mean\")\n",
    "ax[1].set_title(\"log_series without rolling mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks considerably better, however one of the primary drawbacks is that the time-period has to be strictly defined.\n",
    "\n",
    "### Weighted Moving Averages\n",
    "\n",
    "In this case we can take yearly averages but in complex situations like forecasting a stock price, it is difficult to come up with a number. So we take a ‘weighted moving average’ where more recent values are given a higher weight. There can be many technique for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n",
    "\n",
    "In Pandas this is implemented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psng_ewm = psng_log.ewm(halflife=12).mean()\n",
    "psng_ewm_diff = psng_log - psng_ewm\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2, figsize=(12,6))\n",
    "ax[0].plot(psng_log, label=\"input\")\n",
    "ax[0].plot(psng_ewm, label=\"ewm 12\", color='r')\n",
    "ax[1].plot(psng_ewm_diff, label=\"ewm_diff\")\n",
    "ax[1].plot(psng_ewm_diff.rolling(12, center=True).mean(), label=\"rolling mean\")\n",
    "ax[1].plot(psng_ewm_diff.rolling(12, center=True).std(), label=\"rolling std\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that we use an 'exponential' weighting, with a *halflife* parameter of 12 to define the amount of exponential decay. This is an assumption which depends on business domain, but this goes into complex theories regarding the data you wish to model. \n",
    "\n",
    "### Estimating & Eliminating Seasonality\n",
    "\n",
    "Simple trend reduction techniques don't work in many cases, particularly ones with *high seasonality*. Let's discuss ways of removing seasonality:\n",
    "\n",
    "1. Differencing - using time lags\n",
    "2. Decomposition\n",
    "\n",
    "For instance we could plot the difference between a timeseries and it's shift to eliminate trend and improve stationarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(psng_log - psng_log.shift(), label=\"x - shift(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **decomposing** approach, both trend and seasonality are modelled separately and the remaining part of the series is returned. \n",
    "\n",
    "We can use some powerful statistical functions to calculate this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomp = seasonal_decompose(psng_log)\n",
    "\n",
    "fig,ax=plt.subplots(4, figsize=(8,10))\n",
    "ax[0].plot(psng_log, label=\"input\", color='k')\n",
    "ax[1].plot(decomp.trend, label=\"trend\", color='r')\n",
    "ax[2].plot(decomp.seasonal, label=\"seasonal\", color='g')\n",
    "ax[3].plot(decomp.resid, label=\"residual\", color='b')\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Shifts\n",
    "\n",
    "Another common time series-specific operation is shifting of data in time. Pandas has two closely related methods for computing this: shift() and tshift() In short, the difference between them is that shift() shifts the data, while tshift() shifts the index. In both cases, the shift is specified in multiples of the frequency.\n",
    "\n",
    "Below we shift using each method by 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(3, figsize=(8,8), sharex=True)\n",
    "z.plot(ax=ax[0], label=\"input\")\n",
    "z.shift(100).plot(ax=ax[1], label=\"shift(100)\")\n",
    "z.tshift(100).plot(ax=ax[2], label=\"tshift(100)\")\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.set_xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag-Plot\n",
    "\n",
    "Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:].\n",
    "\n",
    "As you see here all we do is shift the timeseries up by 1 (the lag) and 2 and compare to at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2, figsize=(12,6))\n",
    "for i in range(2):\n",
    "    ax[i].scatter(x[0], x[0].shift(i+1), c='g')\n",
    "    ax[i].set_xlabel(\"$x_t$\")\n",
    "    ax[i].set_ylabel(\"$x_{t+%d}$\" % (i+1))\n",
    "    ax[i].set_title(\"Shift %d\" % (i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation and Partial Autocorrelation plot\n",
    "\n",
    "We saw different techniques and all of them worked reasonably well for making the TS stationary. Lets make model on the TS after differencing as it is a very popular technique. Also, its relatively easier to add noise and seasonality back into predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:\n",
    "\n",
    "- A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\n",
    "- A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.\n",
    "\n",
    "We use two plots to determine these numbers. Lets discuss them first.\n",
    "\n",
    "- **Autocorrelation Function (ACF)**: It is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n",
    "- **Partial Autocorrelation Function (PACF)**: This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "def plot_acf(ts, fig, plot_loc):\n",
    "    lag_acf = acf(ts, nlags=20)\n",
    "    ax = fig.add_subplot(plot_loc)\n",
    "    ax.plot(lag_acf)\n",
    "    ax.axhline(y=0, linestyle=\"--\", color='gray')\n",
    "    ax.axhline(y=-1.96/np.sqrt(len(lag_acf)),linestyle='--',color='gray')\n",
    "    ax.axhline(y=1.96/np.sqrt(len(lag_acf)),linestyle='--',color='gray')\n",
    "    ax.set_title(\"Aurocorrelation function\")\n",
    "    \n",
    "\n",
    "def plot_pacf(ts, fig, plot_loc):\n",
    "    lag_pacf = pacf(ts, nlags=20, method='ols')\n",
    "    ax = fig.add_subplot(plot_loc)\n",
    "    ax.plot(lag_pacf)\n",
    "    ax.axhline(y=0, linestyle=\"--\", color='gray')\n",
    "    ax.axhline(y=-1.96/np.sqrt(len(lag_pacf)),linestyle='--',color='gray')\n",
    "    ax.axhline(y=1.96/np.sqrt(len(lag_pacf)),linestyle='--',color='gray')\n",
    "    ax.set_title(\"Partial Aurocorrelation function\")\n",
    "    \n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "    \n",
    "plot_acf((psng_log - psng_log.shift()).dropna(), fig, 121)\n",
    "plot_pacf((psng_log - psng_log.shift()).dropna(), fig, 122)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the two dotted lines on either sides of 0 are the confidence interevals. We are interested in where the line passes the first confidence interval as this is the number of parameters to take with respect to autoregression (ACF) and moving average (PACF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "**RAINFALL**\n",
    "\n",
    "We're going to be working with some national weather data from the area of *Nashville* from the [United States](https://w2.weather.gov/climate/index.php?wfo=ohx). \n",
    "\n",
    "This data records the amount of rainfall dating back to 1871, every month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = pd.read_table(\"nashville_precip.txt\")\n",
    "rain.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We need to convert this data where rows represent the month, with a datetime object as the index, with rainfall as our variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetimeindex using start and end, using month frequency\n",
    "ts = pd.date_range(\"1/1/1871\", \"1/1/2012\", freq=\"M\")\n",
    "ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert rainfall to long-form series and set the new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year, melt columns and set index.\n",
    "long_rain = (rain.drop(\"Year\", axis=1).melt(var_name=\"month\", value_name=\"rainfall\")\n",
    "                 .set_index(ts).rainfall.astype(np.float))\n",
    "long_rain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly visualise a portion of our data to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_rain[\"2000\":].plot(figsize=(14,8), marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 1.\n",
    "\n",
    "In order to make the data more readable, resample it into years, taking the *median* value to reduce outlier skew.\n",
    "\n",
    "Plot this resampled data against time between 1920 to 1960. Ensure to label all axes and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "\n",
    "Remove the trend/seasonality from the dataset. This can be achieved in a number of ways, but the easiest would be by *differencing*.\n",
    "\n",
    "Plot the monthly rainfall, next to the 'differenced rainfall' with a rolling mean and standard deviation with $w=30$, in row-wise fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.\n",
    "\n",
    "Calculate an *exponential weighted moving average* for the *yearly rainfall* and plot this with the original data, alongside a plot with the EMWA removed, alongside the rolling mean and standard deviation. How well does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.\n",
    "\n",
    "Perform seasonal analysis on the monthly data between **2000**-**present** and output the 4 plots as shown previously. Use one of the previous methods as input (whichever you think is closest to a stationary series). How much of this data can be broken down into trend and seasonality? Which year has the most unpredicted changes in rainfall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your codes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
